for(k in 1:50){
t = 1/k
start = Sys.time()
b1 = b - t*(t(X)%*%(X%*%b - Y) + lambda*sign(b))/nrow(X)
en = Sys.time()
b = b1
loss[k, 1] = sum((X%*%b1 - Y)^2)
times[k, 1] = difftime(en, start, units("secs"))
}
### proximal gradient
softthres = function(v, lambda){
v1 = abs(v)-lambda
return(sign(v)*v1*(v1>0))
}
b = rep(0, 100)
t = 1/max(eigen(t(X)%*%X)$values)
lambda = 0.01
for(k in 1:50){
start = Sys.time()
b1 = softthres(b - t*t(X)%*%(X%*%b - Y), t*lambda)
en = Sys.time()
b = b1
loss[k, 2] = sum((X%*%b1 - Y)^2)
times[k, 2] = difftime(en, start, units("secs"))
}
### accelerated proximal
b = rep(0, 100)
lambda = 0.1
x0 = b
s0 = 1
for(k in 1:50){
start = Sys.time()
x1 = softthres(b - t*t(X)%*%(X%*%b - Y), t*lambda)
s1 = (1+sqrt(1+4*s0^2))/2
b = x1 + (s0 -1)/s1 *(x1 - x0)
en = Sys.time()
x0 = x1
s0 = s1
loss[k, 3] = sum((X%*%b - Y)^2)
times[k, 3] = difftime(en, start, units("secs"))
}
for(i in 1:3){
l = loss[,i]
if(i == 1){
plot(1:length(l),log(l), xlab = "iteration", ylab = "log loss", type = "l", col = i+1)
}else{
lines(1:length(l),log(l), type = "l", col = i+1)
}
}
for(i in 1:3){
l = loss[,i]
if(i == 1){
plot(1:length(l),log(l), xlab = "iteration", ylab = "log loss", type = "l", col = i+1, ylim = range(log(loss[,2])))
}else{
lines(1:length(l),log(l), type = "l", col = i+1)
}
}
legend("topright", col = 2:4, legend = c("Subgradient", "Proximal", "Accelerated Proximal"), lty = 1)
b
b = rep(0, 100)
lambda = 1
for(k in 1:20){
t = 1/k
start = Sys.time()
b1 = b - t*(t(X)%*%(X%*%b - Y) + lambda*sign(b))/nrow(X)
en = Sys.time()
b = b1
loss[k, 1] = sum((X%*%b1 - Y)^2)
times[k, 1] = difftime(en, start, units("secs"))
}
### proximal gradient
softthres = function(v, lambda){
v1 = abs(v)-lambda
return(sign(v)*v1*(v1>0))
}
b = rep(0, 100)
t = 1/max(eigen(t(X)%*%X)$values)
lambda = 1
for(k in 1:20){
start = Sys.time()
b1 = softthres(b - t*t(X)%*%(X%*%b - Y), t*lambda)
en = Sys.time()
b = b1
loss[k, 2] = sum((X%*%b1 - Y)^2)
times[k, 2] = difftime(en, start, units("secs"))
}
### accelerated proximal
b = rep(0, 100)
lambda = 1
x0 = b
s0 = 1
for(k in 1:20){
start = Sys.time()
x1 = softthres(b - t*t(X)%*%(X%*%b - Y), t*lambda)
s1 = (1+sqrt(1+4*s0^2))/2
b = x1 + (s0 -1)/s1 *(x1 - x0)
en = Sys.time()
x0 = x1
s0 = s1
loss[k, 3] = sum((X%*%b - Y)^2)
times[k, 3] = difftime(en, start, units("secs"))
}
for(i in 1:3){
l = loss[,i]
if(i == 1){
plot(1:length(l),log(l), xlab = "iteration", ylab = "log loss", type = "l", col = i+1, ylim = range(log(loss[,2])))
}else{
lines(1:length(l),log(l), type = "l", col = i+1)
}
}
legend("topright", col = 2:4, legend = c("Subgradient", "Proximal", "Accelerated Proximal"), lty = 1)
head(loss)
l = log(loss[,2])
which(l < min(l) +1e-2)[1]
which(log(loss[,2]) < min(log(loss[,1])))[1]
which(log(loss[,3]) < min(log(loss[,1])))[1]
sum(times[,1])
sum(times[1:13,2])
sum(times[1:5,3])
apply(times,2, mean)
apply(times[14:50],2, mean)
sum(times[14:50,2])
for(i in 1:3){
l = times[,i]
if(i == 1){
plot(1:length(l),log(l), xlab = "iteration", ylab = "log loss", type = "l", col = i+1, ylim = range(log(times[,2])))
}else{
lines(1:length(l),log(l), type = "l", col = i+1)
}
}
for(i in 1:3){
l = times[,i]
if(i == 1){
plot(1:length(l),l, xlab = "iteration", ylab = "log loss", type = "l", col = i+1, ylim = range(times[,2]))
}else{
lines(1:length(l),l, type = "l", col = i+1)
}
}
q()
#### calculate log-likelihood
loglikelihood = function(X,Y,beta){
sig = 1/(1+exp(-X%*%beta))
Y = (1+Y)/2
g = -Y*log(sig)-(1-Y)*log(1-sig)
g[is.na(g)] = 0
return(mean(g))
}
#### calculate gradient
grad = function(X,Y,beta){
sig = 1/(1+exp(-X%*%beta))
Y = (1+Y)/2
gradient = -t(X)%*%(Y-sig)/length(Y)
return(gradient)
}
#### backtracking line search
backtracking = function(X, Y, beta, gradient){
t = 1
alpha = 0.3
b = 0.5
g = loglikelihood(X, Y, beta)
repeat{
if(loglikelihood(X, Y, beta - t*gradient)< g - alpha*t*sum(gradient^2)){
break
}else{
t = b*t
}
}
return(t)
}
proximal_oprator = function(t, lambda, beta, group){
d = table(group)
label = as.numeric(names(d))
for(i in 1:length(label)){
b = beta[group = label[i]]
l = 1 -lambda*t*sqrt(d[i])/sqrt(sum(b^2))
beta[group = label[i]] = (l>0)*l*b
}
return(beta)
}
#### b here is the actual values of beta used to calculate error
PGD = function(X, y, b, lambda, group){
beta = rep(0, ncol(X))
error = vector()
repeat{
error = c(error, sum((b-beta)^2))
gradient = grad(X, y, beta)
t = backtracking(X, y, beta, gradient)
beta1 = proximal_oprator(t, lambda, beta - t*gradient, group)
if(loglikelihood(X,y,beta)-loglikelihood(X,y,beta1)<1e-6){
break
}
beta = beta1
}
return(list(beta = beta, error = error/error[1]))
}
PGD_ac = function(X, y, b, lambda, group){
beta = rep(0, ncol(X))
error = vector()
gam0 = beta
s0 = 1
repeat{
error = c(error, sum((b-beta)^2))
gradient = grad(X, y, beta)
t = backtracking(X, y, beta, gradient)
s1 = (1+sqrt(1+4*s0^2))/2
gam1 = proximal_oprator(t, lambda, beta - t*gradient, group)
beta1 = gam1 + (s0-1)/s1*(gam1-gam0)
if(loglikelihood(X,y,beta)-loglikelihood(X,y,beta1)<1e-6){
break
}
beta = beta1
s0 = s1
gam0 = gam1
}
return(list(beta = beta, error = error/error[1]))
}
### beta is initiated in a group form
b = rep(0:4, each = 10) + 0.1*rnorm(50)
group = rep(1:5, each = 10)
n=100
i=1
X = matrix(rnorm(50*n), ncol = 50)
y = 2*rbinom(n, 1, 1/(1+exp(-X%*%b)))-1
res = PGD(X, y, b, 0.1, group)
plot(1:length(res$error), res$error)
res_ac = PGD_ac(1:length(res_ac$error), res_ac$error)
res_ac = PGD_ac(X, y, b, 0.1, group)
plot(1:length(res_ac$error), res_ac$error))
plot(1:length(res_ac$error), res_ac$error)
sign(-1:1)
sign(-1:2)
grad_h = function(beta, group, mu){
d = table(group)
label = as.numeric(names(d))
grad1 = rep(0, length(beta))
for(i in 1:length(label)){
index = (group = label[i])
b = beta[index]
if(sum(b^2)<=d[i]*mu^2){
grad1[index] = b/mu
}else{
grad1[index] = sqrt(d[i])*sign(b)
}
}
return(grad1)
}
GD_smooth_ac = function(X, y, b, lambda, group){
d = table(group)
mu = max(d)/2
beta = rep(0, ncol(X))
error = vector()
gam0 = beta
k = 0
repeat{
error = c(error, sum((b-beta)^2))
gradient = grad(X, y, beta) + grad_h(beta, group, mu)
t = backtracking(X, y, beta, gradient)
gam1 = beta - t*gradient
beta1 = gam1 + k/(k+3)*(gam1-gam0)
if(loglikelihood(X,y,beta)-loglikelihood(X,y,beta1)<1e-6){
break
}
beta = beta1
gam0 = gam1
k = k + 1
}
return(list(beta = beta, error = error/error[1]))
}
grad_h = function(beta, group, mu){
d = table(group)
label = as.numeric(names(d))
grad1 = rep(0, length(beta))
for(i in 1:length(label)){
index = (group = label[i])
b = beta[index]
if(sum(b^2)<=d[i]*mu^2){
grad1[index] = b/mu
}else{
grad1[index] = sqrt(d[i])*sign(b)
}
}
return(grad1)
}
GD_smooth_ac = function(X, y, b, lambda, group){
d = table(group)
mu = max(d)/2
beta = rep(0, ncol(X))
error = vector()
gam0 = beta
k = 0
repeat{
error = c(error, sum((b-beta)^2))
gradient = grad(X, y, beta) + grad_h(beta, group, mu)
t = backtracking(X, y, beta, gradient)
gam1 = beta - t*gradient
beta1 = gam1 + k/(k+3)*(gam1-gam0)
if(loglikelihood(X,y,beta)-loglikelihood(X,y,beta1)<1e-6){
break
}
beta = beta1
gam0 = gam1
k = k + 1
}
return(list(beta = beta, error = error/error[1]))
}
res_gd_ac = GD_smooth_ac(X, y, b, 0.1, group)
plot(1:length(res_gd_ac$error), res_gd_ac$error)
b = rep(0:4, each = 10) + 0.1*rnorm(50)
group = rep(1:5, each = 10)
n = 100
X = matrix(rnorm(50*n), ncol = 50)
y = 2*rbinom(n, 1, 1/(1+exp(-X%*%b)))-1
res = PGD(X, y, b, 0.1, group)
plot(1:length(res$error), res$error)
res_ac = PGD_ac(X, y, b, 0.1, group)
lines(1:length(res_ac$error), res_ac$error)
res_gd_ac = GD_smooth_ac(X, y, b, 0.1, group)
lines(1:length(res_gd_ac$error), res_gd_ac$error)
plot(1:length(res$error), res$error, xlim = c(0,500), type = "l")
lines(1:length(res_ac$error), res_ac$error,type = "l")
lines(1:length(res_gd_ac$error), res_gd_ac$error,type = "l")
plot(1:length(res$error), res$error, xlim = c(0,300), type = "l", ylab = "loss_ratio", xlab = "iteration")
lines(1:length(res_ac$error), res_ac$error,type = "l", col = 2, lty = 2)
lines(1:length(res_gd_ac$error), res_gd_ac$error,type = "l", col = 3, lty = 3)
legend("topright", legend = c("proximal gradient", "accelerated proximal gradient", "accelerated gradient smooth"), col = 1:3, lty = 1:3)
q()
install.packages("haven")
library(haven)
install.packages("rlanb")
install.packages("rlang")
install.packages("rlang")
install.packages("haven")
source('C:/Users/ycsuf/Box/Multiple Stephenson/Code20221005/Fun_multiple_stephenson.R')
library(gurobi)
library(devtools)
install_github("Yongchang-Su/QIoT", force = TRUE) #if needed
# libraries for simulation data generation
library(psych)
library(randomizr)
com_working = function(Z, Y, block, c, methods.list.all){
if(!is.factor(block)){
block = as.factor(block)
}
total.list = list()
n = length(methods.list.all)
N = length(Y)
B = length(levels(block))
block.levels = levels(block)
## calculate numbers of observations in each block
nb = rep(NA, B)
for(i in 1:B){
nb[i] = sum(block == block.levels[i])
}
for (l in 1:n){
method.list.all = methods.list.all[[l]]
Tlist = list()
for(i in 1:B){
Zb = Z[block == block.levels[i]]
Yb = Y[block == block.levels[i]]
Ti = matrix(nrow = 2, ncol = nb[i] + 1)
Ti[1,] = 0:nb[i]
for(ii in 0:nb[i]){
if(length(method.list.all)==1){
method.list = method.list.all[[1]]
}else{
method.list = method.list.all[[i]]
}
Ti[2, ii+1] = min_stat(Zb, Yb, nb[i]-ii, c, method.list = method.list)
}
Tlist[[i]] = Ti
}
total.list[[l]] <- Tlist
}
return(total.list)
}
########################################################
### function for implement Gurobi for our ILP problem
########################################################
## two major changes:
##  (1) change in objective function statement
##  (2) allow eta to have negative value
Gurobi_sol_com <- function(coeflists, p, exact = TRUE){
model = list()
Q = NULL
H = length(coeflists)
B = length(coeflists[[1]])
nb = vector(length = B)
for (h in 1 : H){
coeflist = coeflists[[h]]
for (i in 1 : B){
Q = c(Q, coeflist[[i]][2,])
}
}
Q1 <- c(rep(0, length(Q)), 1)
#  Q1 <- c(Q, 1)
A_final <- c(Q, -1)    ## will add this as a final constraint
for (i in 1 : B){
nb[i] = ncol(coeflist[[i]]) ## this does not change by choice of SRSS(Stratified Rank Sum Statistic)
}
HB = H * B
nb2 = rep(nb, length(coeflists))  ## added
n = length(Q1)
A = matrix(0, nrow = H * (B + 1) + 1, ncol = n) ## added one row one column each
indx = c(0, cumsum(nb2))
for (i in 1 : HB){
A[i, (indx[i] + 1):indx[i+1]] = 1
}
for (h in 1 : H){
for (i in 1 : B) {
A[HB + h,  ((h-1) * sum(nb)+1) : (h * sum(nb))]  =
rep(coeflist[[i]][1,], B)
}
}
A[H * (B+1) + 1, ] = A_final
model$A = A
model$obj = Q1
model$modelsense = "min"
model$rhs = c(rep(1, HB), rep(p, H), 0)
model$sense = c(rep("=", HB), rep("<=", H), "<=" )
model$lb = c(rep(0, length(Q)), -Inf)
#  model$vtype = vec(B = length = length(Q1))
if(exact){
model$vtype = c(rep("B", length(Q)), "C")
}
gurobi(model)           ## can omit this when we only want to check the output, not the optimization process
params <- list(OutputFlag = 0)
result = gurobi::gurobi(model,params)
return(list(sol = result$x, obj = result$objval))
}
############################################
# test whether functions works
############################################
# simulation data generation
s = 20
n = 50
m = 0.5 * n
N = s * n
k = ceiling(0.9 * N)
p = N - k
c = 0
simulation_Z <- as.data.frame(block.random(n = N, c(block = s)))
simulation_Z$block <- as.factor(simulation_Z$block)
head(simulation_Z)
levels(simulation_Z$block)
test_Z <- block_ra(blocks = simulation_Z$block)
table(test_Z, simulation_Z$block)
Y1 = 2 + rnorm(N)
Y0 = rnorm(N)
Z = block_ra(blocks = simulation_Z$block)
Y = Z * Y1 + (1-Z) * Y0
Z_block <- simulation_Z$block
# testing methods; for new rank_score method
method.list.all.4 <- list()
method.list.4 = list(name = "Stephenson",
s = 15
)
method.list.all.4[[1]] <- method.list.4
methods.list.all <- list(method.list.all.4)
# check whether our function works
coeflists = com_working(Z, Y, block = Z_block, c = c, methods.list.all = methods.list.all)
Gurobi_sol_com(coeflists, p, exact = TRUE)
method.list.4 = list(name = "Stephenson",
s = 15,
standardize = TRUE
)
method.list.all.4[[1]] <- method.list.4
methods.list.all <- list(method.list.all.4)
# check whether our function works
coeflists = com_working(Z, Y, block = Z_block, c = c, methods.list.all = methods.list.all)
Gurobi_sol_com(coeflists, p, exact = TRUE)
method.list.4 = list(name = "Stephenson",
s = 15,
standardize = FALSE
)
method.list.all.4[[1]] <- method.list.4
methods.list.all <- list(method.list.all.4)
# check whether our function works
coeflists = com_working(Z, Y, block = Z_block, c = c, methods.list.all = methods.list.all)
Gurobi_sol_com(coeflists, p, exact = TRUE)
QIoT::min_stat_block(Z, Y, block = Z_block, k, c, method.list.all = method.list.all.4, opt.method = "Greedy" )
coeflists = test_stat_matrix_block(Z, Y, Z_block, 0, method.list.all.4)
QIoT::min_stat_block(Z, Y, block = Z_block, k, c, method.list.all = method.list.all.4, opt.method = "Greedy" )
QIoT::min_stat_block(Z, Y, block = Z_block, k, c, method.list.all = method.list.all.4, opt.method = "ILP_gurobi" )
source('C:/Users/ycsuf/Box/Stratified Quantile/Code_2nd_Submission/RI_Quantile.R')
LpGreedy_On(coeflists, p)$obj
coeflists = test_stat_matrix_block(Z, Y, Z_block, 0, method.list.all.4)
LpGreedy_On(coeflists, p)$obj
method.list.all.4 <- list()
for(i in 1:s){
method.list.4 = list(name = "Stephenson",
s = 15,
standardize = FALSE
)
method.list.all.4[[i]] <- method.list.4
}
QIoT::min_stat_block(Z, Y, block = Z_block, k, c, method.list.all = method.list.all.4, opt.method = "ILP_gurobi" )
Rcpp::sourceCpp("test.cpp")
setwd("C:/Users/ycsuf/Box/Stratified Quantile")
Rcpp::sourceCpp("test.cpp")
coeflists =
Rcpp::sourceCpp("test.cpp")
coeflists =
Rcpp::sourceCpp("test.cpp")
test_stat_matrix_block(Z, Y, Z_block, 0, method.list.all.4)
setwd("C:/Users/ycsuf/Box/Stratified Quantile/QIoT")
library(devtools)
document()
unload("QIoT")
unload("QIoT")
library(gurobi)
library(devtools)
install_github("Yongchang-Su/QIoT", force = TRUE) #if needed
