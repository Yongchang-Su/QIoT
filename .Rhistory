s1 = (1+sqrt(1+4*s0^2))/2
b = x1 + (s0 -1)/s1 *(x1 - x0)
en = Sys.time()
x0 = x1
s0 = s1
loss[k, 3] = sum((X%*%b - Y)^2)
times[k, 3] = difftime(en, start, units("secs"))
}
for(i in 1:3){
l = loss[,i]
if(i == 1){
plot(1:length(l),log(l), xlab = "iteration", ylab = "log loss", type = "l", col = i+1)
}else{
lines(1:length(l),log(l), type = "l", col = i+1)
}
}
legend("topright", col = 2:4, legend = c("Subgradient", "Proximal", "Accelerated Proximal"), lty = 1)
times = matrix(nrow = 20, ncol = 3)
loss = times
### subgradient
b = rep(0, 100)
lambda = 0.1
for(k in 1:20){
t = 1/k
start = Sys.time()
b1 = b - t*(t(X)%*%(X%*%b - Y) + lambda*sign(b))/nrow(X)
en = Sys.time()
b = b1
loss[k, 1] = sum((X%*%b1 - Y)^2)
times[k, 1] = difftime(en, start, units("secs"))
}
### proximal gradient
softthres = function(v, lambda){
v1 = abs(v)-lambda
return(sign(v)*v1*(v1>0))
}
b = rep(0, 100)
t = 1/max(eigen(t(X)%*%X)$values)
lambda = 0.01
for(k in 1:20){
start = Sys.time()
b1 = softthres(b - t*t(X)%*%(X%*%b - Y), t*lambda)
en = Sys.time()
b = b1
loss[k, 2] = sum((X%*%b1 - Y)^2)
times[k, 2] = difftime(en, start, units("secs"))
}
### accelerated proximal
b = rep(0, 100)
lambda = 0.1
x0 = b
s0 = 1
for(k in 1:20){
start = Sys.time()
x1 = softthres(b - t*t(X)%*%(X%*%b - Y), t*lambda)
s1 = (1+sqrt(1+4*s0^2))/2
b = x1 + (s0 -1)/s1 *(x1 - x0)
en = Sys.time()
x0 = x1
s0 = s1
loss[k, 3] = sum((X%*%b - Y)^2)
times[k, 3] = difftime(en, start, units("secs"))
}
for(i in 1:3){
l = loss[,i]
if(i == 1){
plot(1:length(l),log(l), xlab = "iteration", ylab = "log loss", type = "l", col = i+1)
}else{
lines(1:length(l),log(l), type = "l", col = i+1)
}
}
legend("topright", col = 2:4, legend = c("Subgradient", "Proximal", "Accelerated Proximal"), lty = 1)
apply(times, mean, 2)
apply(times, 2, mean)
l = koss[,2]
l = loss[,2]
which(l == min(l))[1]
which(l < min(l) +1e-5)[1]
which(l < min(l) +1e-3)[1]
l = log(loss[,2])
which(l < min(l) +1e-5)[1]
l = log(loss[,3])
which(l < min(l) +1e-5)[1]
which(l < min(l) +1e-3)[1]
which(l < min(l) +1e-2)[1]
l = log(loss[,2])
which(l < min(l) +1e-2)[1]
X = matrix(rnorm(100*200), ncol = 100)
beta = rep(0, 100)
beta[1:5] = 1
Y = X%*%beta + rnorm(200)
times = matrix(nrow = 20, ncol = 3)
loss = times
### subgradient
b = rep(0, 100)
lambda = 0.1
for(k in 1:20){
t = 1/k
start = Sys.time()
b1 = b - t*(t(X)%*%(X%*%b - Y) + lambda*sign(b))/nrow(X)
en = Sys.time()
b = b1
loss[k, 1] = sum((X%*%b1 - Y)^2)
times[k, 1] = difftime(en, start, units("secs"))
}
### proximal gradient
softthres = function(v, lambda){
v1 = abs(v)-lambda
return(sign(v)*v1*(v1>0))
}
b = rep(0, 100)
t = 1/max(eigen(t(X)%*%X)$values)
lambda = 0.01
for(k in 1:20){
start = Sys.time()
b1 = softthres(b - t*t(X)%*%(X%*%b - Y), t*lambda)
en = Sys.time()
b = b1
loss[k, 2] = sum((X%*%b1 - Y)^2)
times[k, 2] = difftime(en, start, units("secs"))
}
### accelerated proximal
b = rep(0, 100)
lambda = 0.1
x0 = b
s0 = 1
for(k in 1:20){
start = Sys.time()
x1 = softthres(b - t*t(X)%*%(X%*%b - Y), t*lambda)
s1 = (1+sqrt(1+4*s0^2))/2
b = x1 + (s0 -1)/s1 *(x1 - x0)
en = Sys.time()
x0 = x1
s0 = s1
loss[k, 3] = sum((X%*%b - Y)^2)
times[k, 3] = difftime(en, start, units("secs"))
}
for(i in 1:3){
l = loss[,i]
if(i == 1){
plot(1:length(l),log(l), xlab = "iteration", ylab = "log loss", type = "l", col = i+1)
}else{
lines(1:length(l),log(l), type = "l", col = i+1)
}
}
legend("topright", col = 2:4, legend = c("Subgradient", "Proximal", "Accelerated Proximal"), lty = 1)
times = matrix(nrow = 50, ncol = 3)
loss = times
### subgradient
b = rep(0, 100)
lambda = 0.1
for(k in 1:50){
t = 1/k
start = Sys.time()
b1 = b - t*(t(X)%*%(X%*%b - Y) + lambda*sign(b))/nrow(X)
en = Sys.time()
b = b1
loss[k, 1] = sum((X%*%b1 - Y)^2)
times[k, 1] = difftime(en, start, units("secs"))
}
### proximal gradient
softthres = function(v, lambda){
v1 = abs(v)-lambda
return(sign(v)*v1*(v1>0))
}
b = rep(0, 100)
t = 1/max(eigen(t(X)%*%X)$values)
lambda = 0.01
for(k in 1:50){
start = Sys.time()
b1 = softthres(b - t*t(X)%*%(X%*%b - Y), t*lambda)
en = Sys.time()
b = b1
loss[k, 2] = sum((X%*%b1 - Y)^2)
times[k, 2] = difftime(en, start, units("secs"))
}
### accelerated proximal
b = rep(0, 100)
lambda = 0.1
x0 = b
s0 = 1
for(k in 1:50){
start = Sys.time()
x1 = softthres(b - t*t(X)%*%(X%*%b - Y), t*lambda)
s1 = (1+sqrt(1+4*s0^2))/2
b = x1 + (s0 -1)/s1 *(x1 - x0)
en = Sys.time()
x0 = x1
s0 = s1
loss[k, 3] = sum((X%*%b - Y)^2)
times[k, 3] = difftime(en, start, units("secs"))
}
for(i in 1:3){
l = loss[,i]
if(i == 1){
plot(1:length(l),log(l), xlab = "iteration", ylab = "log loss", type = "l", col = i+1)
}else{
lines(1:length(l),log(l), type = "l", col = i+1)
}
}
for(i in 1:3){
l = loss[,i]
if(i == 1){
plot(1:length(l),log(l), xlab = "iteration", ylab = "log loss", type = "l", col = i+1, ylim = range(log(loss[,2])))
}else{
lines(1:length(l),log(l), type = "l", col = i+1)
}
}
legend("topright", col = 2:4, legend = c("Subgradient", "Proximal", "Accelerated Proximal"), lty = 1)
b
b = rep(0, 100)
lambda = 1
for(k in 1:20){
t = 1/k
start = Sys.time()
b1 = b - t*(t(X)%*%(X%*%b - Y) + lambda*sign(b))/nrow(X)
en = Sys.time()
b = b1
loss[k, 1] = sum((X%*%b1 - Y)^2)
times[k, 1] = difftime(en, start, units("secs"))
}
### proximal gradient
softthres = function(v, lambda){
v1 = abs(v)-lambda
return(sign(v)*v1*(v1>0))
}
b = rep(0, 100)
t = 1/max(eigen(t(X)%*%X)$values)
lambda = 1
for(k in 1:20){
start = Sys.time()
b1 = softthres(b - t*t(X)%*%(X%*%b - Y), t*lambda)
en = Sys.time()
b = b1
loss[k, 2] = sum((X%*%b1 - Y)^2)
times[k, 2] = difftime(en, start, units("secs"))
}
### accelerated proximal
b = rep(0, 100)
lambda = 1
x0 = b
s0 = 1
for(k in 1:20){
start = Sys.time()
x1 = softthres(b - t*t(X)%*%(X%*%b - Y), t*lambda)
s1 = (1+sqrt(1+4*s0^2))/2
b = x1 + (s0 -1)/s1 *(x1 - x0)
en = Sys.time()
x0 = x1
s0 = s1
loss[k, 3] = sum((X%*%b - Y)^2)
times[k, 3] = difftime(en, start, units("secs"))
}
for(i in 1:3){
l = loss[,i]
if(i == 1){
plot(1:length(l),log(l), xlab = "iteration", ylab = "log loss", type = "l", col = i+1, ylim = range(log(loss[,2])))
}else{
lines(1:length(l),log(l), type = "l", col = i+1)
}
}
legend("topright", col = 2:4, legend = c("Subgradient", "Proximal", "Accelerated Proximal"), lty = 1)
head(loss)
l = log(loss[,2])
which(l < min(l) +1e-2)[1]
which(log(loss[,2]) < min(log(loss[,1])))[1]
which(log(loss[,3]) < min(log(loss[,1])))[1]
sum(times[,1])
sum(times[1:13,2])
sum(times[1:5,3])
apply(times,2, mean)
apply(times[14:50],2, mean)
sum(times[14:50,2])
for(i in 1:3){
l = times[,i]
if(i == 1){
plot(1:length(l),log(l), xlab = "iteration", ylab = "log loss", type = "l", col = i+1, ylim = range(log(times[,2])))
}else{
lines(1:length(l),log(l), type = "l", col = i+1)
}
}
for(i in 1:3){
l = times[,i]
if(i == 1){
plot(1:length(l),l, xlab = "iteration", ylab = "log loss", type = "l", col = i+1, ylim = range(times[,2]))
}else{
lines(1:length(l),l, type = "l", col = i+1)
}
}
q()
#### calculate log-likelihood
loglikelihood = function(X,Y,beta){
sig = 1/(1+exp(-X%*%beta))
Y = (1+Y)/2
g = -Y*log(sig)-(1-Y)*log(1-sig)
g[is.na(g)] = 0
return(mean(g))
}
#### calculate gradient
grad = function(X,Y,beta){
sig = 1/(1+exp(-X%*%beta))
Y = (1+Y)/2
gradient = -t(X)%*%(Y-sig)/length(Y)
return(gradient)
}
#### backtracking line search
backtracking = function(X, Y, beta, gradient){
t = 1
alpha = 0.3
b = 0.5
g = loglikelihood(X, Y, beta)
repeat{
if(loglikelihood(X, Y, beta - t*gradient)< g - alpha*t*sum(gradient^2)){
break
}else{
t = b*t
}
}
return(t)
}
proximal_oprator = function(t, lambda, beta, group){
d = table(group)
label = as.numeric(names(d))
for(i in 1:length(label)){
b = beta[group = label[i]]
l = 1 -lambda*t*sqrt(d[i])/sqrt(sum(b^2))
beta[group = label[i]] = (l>0)*l*b
}
return(beta)
}
#### b here is the actual values of beta used to calculate error
PGD = function(X, y, b, lambda, group){
beta = rep(0, ncol(X))
error = vector()
repeat{
error = c(error, sum((b-beta)^2))
gradient = grad(X, y, beta)
t = backtracking(X, y, beta, gradient)
beta1 = proximal_oprator(t, lambda, beta - t*gradient, group)
if(loglikelihood(X,y,beta)-loglikelihood(X,y,beta1)<1e-6){
break
}
beta = beta1
}
return(list(beta = beta, error = error/error[1]))
}
PGD_ac = function(X, y, b, lambda, group){
beta = rep(0, ncol(X))
error = vector()
gam0 = beta
s0 = 1
repeat{
error = c(error, sum((b-beta)^2))
gradient = grad(X, y, beta)
t = backtracking(X, y, beta, gradient)
s1 = (1+sqrt(1+4*s0^2))/2
gam1 = proximal_oprator(t, lambda, beta - t*gradient, group)
beta1 = gam1 + (s0-1)/s1*(gam1-gam0)
if(loglikelihood(X,y,beta)-loglikelihood(X,y,beta1)<1e-6){
break
}
beta = beta1
s0 = s1
gam0 = gam1
}
return(list(beta = beta, error = error/error[1]))
}
### beta is initiated in a group form
b = rep(0:4, each = 10) + 0.1*rnorm(50)
group = rep(1:5, each = 10)
n=100
i=1
X = matrix(rnorm(50*n), ncol = 50)
y = 2*rbinom(n, 1, 1/(1+exp(-X%*%b)))-1
res = PGD(X, y, b, 0.1, group)
plot(1:length(res$error), res$error)
res_ac = PGD_ac(1:length(res_ac$error), res_ac$error)
res_ac = PGD_ac(X, y, b, 0.1, group)
plot(1:length(res_ac$error), res_ac$error))
plot(1:length(res_ac$error), res_ac$error)
sign(-1:1)
sign(-1:2)
grad_h = function(beta, group, mu){
d = table(group)
label = as.numeric(names(d))
grad1 = rep(0, length(beta))
for(i in 1:length(label)){
index = (group = label[i])
b = beta[index]
if(sum(b^2)<=d[i]*mu^2){
grad1[index] = b/mu
}else{
grad1[index] = sqrt(d[i])*sign(b)
}
}
return(grad1)
}
GD_smooth_ac = function(X, y, b, lambda, group){
d = table(group)
mu = max(d)/2
beta = rep(0, ncol(X))
error = vector()
gam0 = beta
k = 0
repeat{
error = c(error, sum((b-beta)^2))
gradient = grad(X, y, beta) + grad_h(beta, group, mu)
t = backtracking(X, y, beta, gradient)
gam1 = beta - t*gradient
beta1 = gam1 + k/(k+3)*(gam1-gam0)
if(loglikelihood(X,y,beta)-loglikelihood(X,y,beta1)<1e-6){
break
}
beta = beta1
gam0 = gam1
k = k + 1
}
return(list(beta = beta, error = error/error[1]))
}
grad_h = function(beta, group, mu){
d = table(group)
label = as.numeric(names(d))
grad1 = rep(0, length(beta))
for(i in 1:length(label)){
index = (group = label[i])
b = beta[index]
if(sum(b^2)<=d[i]*mu^2){
grad1[index] = b/mu
}else{
grad1[index] = sqrt(d[i])*sign(b)
}
}
return(grad1)
}
GD_smooth_ac = function(X, y, b, lambda, group){
d = table(group)
mu = max(d)/2
beta = rep(0, ncol(X))
error = vector()
gam0 = beta
k = 0
repeat{
error = c(error, sum((b-beta)^2))
gradient = grad(X, y, beta) + grad_h(beta, group, mu)
t = backtracking(X, y, beta, gradient)
gam1 = beta - t*gradient
beta1 = gam1 + k/(k+3)*(gam1-gam0)
if(loglikelihood(X,y,beta)-loglikelihood(X,y,beta1)<1e-6){
break
}
beta = beta1
gam0 = gam1
k = k + 1
}
return(list(beta = beta, error = error/error[1]))
}
res_gd_ac = GD_smooth_ac(X, y, b, 0.1, group)
plot(1:length(res_gd_ac$error), res_gd_ac$error)
b = rep(0:4, each = 10) + 0.1*rnorm(50)
group = rep(1:5, each = 10)
n = 100
X = matrix(rnorm(50*n), ncol = 50)
y = 2*rbinom(n, 1, 1/(1+exp(-X%*%b)))-1
res = PGD(X, y, b, 0.1, group)
plot(1:length(res$error), res$error)
res_ac = PGD_ac(X, y, b, 0.1, group)
lines(1:length(res_ac$error), res_ac$error)
res_gd_ac = GD_smooth_ac(X, y, b, 0.1, group)
lines(1:length(res_gd_ac$error), res_gd_ac$error)
plot(1:length(res$error), res$error, xlim = c(0,500), type = "l")
lines(1:length(res_ac$error), res_ac$error,type = "l")
lines(1:length(res_gd_ac$error), res_gd_ac$error,type = "l")
plot(1:length(res$error), res$error, xlim = c(0,300), type = "l", ylab = "loss_ratio", xlab = "iteration")
lines(1:length(res_ac$error), res_ac$error,type = "l", col = 2, lty = 2)
lines(1:length(res_gd_ac$error), res_gd_ac$error,type = "l", col = 3, lty = 3)
legend("topright", legend = c("proximal gradient", "accelerated proximal gradient", "accelerated gradient smooth"), col = 1:3, lty = 1:3)
q()
install.packages("haven")
library(haven)
install.packages("rlanb")
install.packages("rlang")
install.packages("rlang")
install.packages("haven")
setwd("C:/Users/ycsuf/Box")
setwd("C:/Users/ycsuf/Box/Stratified Quantile")
library(devtools)
install("QIoT")
update.packages("devtools")
find_rtools(T)
library(QIoT)
QIoT::block_conf_quant_larger
install.packages("gurobi")
library(gurobi)
install.packages('c:/gurobi952/win64/R/gurobi_9.5-2.zip', repos=NULL)
require(gurobi)
a = require(gurobi)
a
f = function(x){
return("error")
}
f(1)
testit <- function() warning("problem in testit", call. = FALSE)
testit
testit()
f = function(){
return(warning("error"))
}
f()
setwd("C:/Users/ycsuf/Box/Stratified Quantile/QIoT")
devtools::document()
document()
document()
